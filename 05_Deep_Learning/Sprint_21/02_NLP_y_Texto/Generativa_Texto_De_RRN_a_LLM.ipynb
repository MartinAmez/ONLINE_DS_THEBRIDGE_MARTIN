{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ8-spwGrtMI"
      },
      "source": [
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/cabecera.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43dkXdozrtMK"
      },
      "source": [
        "## Una (muy) breve (pero densa) introducción a la IA Generativa en Textos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4Tab-WKrtMK"
      },
      "source": [
        "*NOTA: Este notebook adapta y amplía parte del capítulo dedicado a NLP en el excelente libro [Hands on Machine Learning for Python](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#nlp_chapter)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-TYG-VJrtML"
      },
      "source": [
        "La generación de textos y más que eso el tratamiento de lenguaje natural ha dado un salto \"cuántico\" en los últimos años dentro del campo de la IA. Vamos a ver de una forma poco ortodoxa la evolución partiendo de las arquitecturas más complejas sobre redes recurrentes hasta terminar en los instruct LLM (la base de la IA multimodal generativa actual). Será un viaje denso y breve, pero espero que despierte el interés en ti para ampliar más con el material extra que se proporciona en la plataforma y en algunos enlaces de este notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTwE2Qu4rtMM"
      },
      "source": [
        "*Antes de empezar, este notebook \"no\" se recomienda si no se ejecuta en un entorno con GPU disponible. Si no es el caso -> Colab:*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUIn-3_qrtMM"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/Jaimegrp/DS_Online_Oct23/blob/main/05_Deep_Learning/Sprint_21/02_NLP_y_Texto/Generativa_Texto_De_RRN_a_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOQNKhDtrtMN",
        "outputId": "ff20cec0-1854-4295-9898-b6b0b7f5a36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n",
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XXGCfC43rtMN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gCHANlswrtMO"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rg025n4NrtMO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6pZ5hfvkrtMP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from collections import Counter\n",
        "from IPython.display import clear_output\n",
        "from pathlib import Path\n",
        "from random import random, randint,sample\n",
        "from time import time, sleep\n",
        "\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oqYnbmiRrtMP"
      },
      "outputs": [],
      "source": [
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
        "              \"accelerator.\")\n",
        "    if \"kaggle_secrets\" in sys.modules:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDnv_Sx0rtMP"
      },
      "source": [
        "## Usando RNNs: Una estructura Encoder-Decoder para traducir de inglés a español"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBcIBBUArtMP"
      },
      "source": [
        "El primer gran avance por encima de la vectorización y las técnicas iniciales de resumen (summarization), traducción (translation), preguntas y respuestas (Q&A), es el empleo de redes recurrentes en el tratamiento de textos. Veamos un ejemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiDAs2-VrtMP"
      },
      "source": [
        "El objetivo del siguiente ejemplo es doble:  \n",
        "1. Mostrar como las redes recurrentes pueden configurarse para tratar un problema de traducción donde a una secuencia de entrada de longitud variable le corresponderá una secuencia de longitud variable y además no necesariamente coincidente, en dicha longitud, con la de entrada. Este problema se aplica a la traducción de inglés a español pero es un esquema que se puede emplear en cualquier tipo de cambio de representación entre secuencias (yes, para pasar de una secuencia a una imagen y viceversa).  \n",
        "\n",
        "2. Introducir de forma progresiva el concepto de Atention (atención) para mejorar el modelo anterior y sobre este la arquitectura conocida como Transformers, que nos permitirá hablar de GPT, BERT y los LLN (Large Language Models, no Master of Laws, ojo) en general."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ81WAyHrtMQ"
      },
      "source": [
        "### El dataset de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh3YfbK_rtMQ"
      },
      "source": [
        "Utilizaremos un datset que empareja palabras y frases en inglés con sus traducciones al español de Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kry1EyVWrtMQ",
        "outputId": "8c954efe-d689-4f3a-f1e4-f01f1bc4e2ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n",
        "                               extract=True)\n",
        "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text(encoding = \"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emipPlQartMQ"
      },
      "source": [
        "Observamos su contenido (siempre hay que mirar la mercancía antes de ponerse a cocinar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PsOOYjNrtMQ",
        "outputId": "6fe4f0a2-b511-4907-cfb6-876ea3c4f539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tVe.\n",
            "Go.\tVete.\n",
            "Go.\tVaya.\n",
            "Go.\tVáyase.\n",
            "Hi.\tHola.\n",
            "Run!\t¡Corre!\n",
            "Run.\tCorred.\n",
            "Who?\t¿Quién?\n",
            "Fire!\t¡Fueg\n"
          ]
        }
      ],
      "source": [
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34GsOC45rtMQ"
      },
      "source": [
        "Lo transformamos para que sea una relación de secuencia a secuencia (dada una secuencia tenemos su target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "o8hoHlQwrtMR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.seed(42)  # extra code – ensures reproducibility on CPU\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs)  # separa las parejas en dos listas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "hr6GoqdzuVq9",
        "outputId": "41e1a3ba-7119-4e7b-fc2c-501e308b7c90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tenemos 118964 sentencias para entrenar\n",
            "Distribuciones del corpus en inglés\n",
            "count    118964.000000\n",
            "mean          6.310363\n",
            "std           2.611586\n",
            "min           1.000000\n",
            "25%           4.000000\n",
            "50%           6.000000\n",
            "75%           8.000000\n",
            "max          47.000000\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(f\"Tenemos {len(sentences_en)} sentencias para entrenar\")\n",
        "print(\"Distribuciones del corpus en inglés\")\n",
        "series_en = pd.Series([len(sentencia.split()) for sentencia in sentences_en])\n",
        "series_en.hist();\n",
        "print(series_en.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "pPbLpRYru4F3",
        "outputId": "6e705c84-606e-48fb-a13a-b8e6c674a97c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribuciones del corpus en español\n",
            "count    118964.000000\n",
            "mean          6.083866\n",
            "std           2.764452\n",
            "min           1.000000\n",
            "25%           4.000000\n",
            "50%           6.000000\n",
            "75%           7.000000\n",
            "max          49.000000\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"Distribuciones del corpus en español\")\n",
        "series_es = pd.Series([len(sentencia.split()) for sentencia in sentences_es])\n",
        "series_es.hist();\n",
        "print(series_es.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wiqv5UaVrtMR",
        "outputId": "f2cae51e-1c01-4c11-e7a2-c53b69512d40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "His opinion is different from mine.(6), '=>', Su opinión es diferente de la mía.(7)\n",
            "Despite everything she said to me, I can't stop thinking about her.(12), '=>', A pesar de todo lo que me dijo, no puedo parar de pensar en ella.(15)\n",
            "No one did anything but dance.(6), '=>', Nadie hizo nada aparte de bailar.(6)\n"
          ]
        }
      ],
      "source": [
        "origen = randint(0,len(sentences_es)-3)\n",
        "for i in range(origen, origen+3):\n",
        "    print(f\"{sentences_en[i]}({len(sentences_en[i].split())}), '=>', {sentences_es[i]}({len(sentences_es[i].split())})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INrVVj9_rtMR"
      },
      "source": [
        "### ENCODER-DECODER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_pkKlb_rtMR"
      },
      "source": [
        "En la sesión de redes recurrentes ya vimos la estructura básica y citamos algún uso de la misma  \n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/encoder_decoder.jpg?raw=1\" alt=\"Diagram of encoder_decoder\" width=\"400\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K6q_NP-rtMR"
      },
      "source": [
        "Y, ¿por qué esta arquitectura? Porque antes de que se propusiese no había forma de entrenar modelos que admitiesen secuencias de longitud variable con target otra secuencia de longitud variable (y por tanto pudiendo ser esa longitud diferente a la primera)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGjjpK26rtMR"
      },
      "source": [
        "El encoder ahora se encarga de convertir cualquier secuencia que haya a la entrada en un vector de longitud fija y el decoder convertira este vector en una secuencia de salida de longitud variable.  \n",
        "\n",
        "De hecho al encoder le vamos a dar de comer secuencias de longitud fija pero lo suficientemente larga como para que entren todas, y aplicaremos el truco del padding para completar y el de la máscara para que no le afecte a las secuencias cortas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__amabJsrtMS"
      },
      "source": [
        "Este es el modelo que vamos a construir (sin \"desenrrollar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLiJdIRZrtMS"
      },
      "source": [
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/encoder_decoder_to_train.jpg?raw=1\" alt=\"Diagram of encoder_decoder\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYk-xYBOrtMS"
      },
      "source": [
        "Mejor si lo desenrrollamos:\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/DS_Online_Oct23/blob/main/05_Deep_Learning/Sprint_21/02_NLP_y_Texto/img/encoder_decoder_unrolled.jpg?raw=1\" alt=\"Diagram of encoder_decoder\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpCMaENGrtMS"
      },
      "source": [
        "Veamos como funcionaría (en entrenamiento) [*nota: h y c son los hidden_state de la recurrente del encoder, $h_d$ y $c_d$ son los hidden_state de la recurrente del decoder*]:\n",
        "1. Al enconder le damos la secuencia [I, like, soccer], y no le va a pasar nada todavía al encoder...\n",
        "2. Hace el embedding, supongamos que de 2 dimensiones, [ (0.212,-3.32), (1.34, 0.344), (6.665,-4.443)]\n",
        "3. Procesa la secuencia uno a uno y va transmitiendose el hidden_state ( y la cell_state, es una LSTM) en cada elemento de la secuencia:\n",
        "    Procesa: e0: [(0.212,-3.32),(0,0,...0),(0,0.....)]\n",
        "    Procesa: e1: [(1.34,0.344), h([(0.212,-3.32),(0,0,...0),(0,0.....)]),c((0.212,-3.32),(0,0,...0),(0,0.....))] (recordad que las LSTM tienen dos estados ocultos h y c el primero en teoría para la memoria a corto y el segundo para la memoria a largo)\n",
        "    Procesa: e2 [(6.665, -4.443), h(e1), c(1)]\n",
        "4. Ahora sí devuelve [salida(e2),h(e2),c(2)] y esto es parte de lo que entra en el Decoder\n",
        "5. El decoder a la vez ha hecho el embedding de su entrada [emb(\"\\<sos\\>\"),emb(\"Me\"),emb(\"gusta\"),emb(\"el\"),emb(\"fútbol\")]\n",
        "5. Lo primero que procesa el decoder es d1: [h(e2),c(e2),emb(\"\\<sos\\>\")] y la capa de salida predice (en el caso de la figura) \"me\"  \n",
        "6. Luego procesa d2: [emb(\"Me\"),$h_d$(d1),$c_d$(d1)] y la capa de salida predice (en este caso): \"encanta\"\n",
        "7. procesa d3: [emb(\"gusta\"),$h_d$(d2),$c_d$(d2)] y la capa de salida predice: \"el\" (Importante, le entra el embedding de la palabra que tendría que haber predicho antes (\"gusta\") no la que realmente predijo \"encanta\", esto es *Teaching Forcing*)\n",
        "8. procesa d4: [emb(\"el\"),$h_d$(d3),$c_d$(d3)] y la capa de salida predice: \"fútbol\"\n",
        "9. procesa d5: [emb(\"fútbol\"),$h_d$(d4),$c_d$(4)] y la capa de salida predice: \"\\<eos\\>\" (end of sequence) (podría haber hecho la predicción de otra palabra y hubiera acabado igual pero se contabilizaría como un error para el optimizador, etc, etc)\n",
        "10. Se acaba la secuencia de entrada para el decoder\n",
        "\n",
        "A destacar:\n",
        "- El encoder sólo le pasara los hidden_state (h y c) del final de la secuencia de entrada al decoder\n",
        "- El decoder trabaja sobre el target completo desplazado una vez (esto nos sirve para construir el vec2seq)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8ADOrEjrtMS"
      },
      "source": [
        "### Construcción del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-mpvJODrtMS"
      },
      "source": [
        "Ok, ahora que todo ha quedado clarito como la teoría de la relatividad, vamos a construir el modelo.  \n",
        "Primero las capas de embeddings: como ya hemos visto primero nuestros vectorizadores para convertir cada sentencia en secuencia de índices y después la capa de embedding para que aprenda cual es la mejor reprensentación de cada índice/palabra en el contexto del problema que estamos resolviendo. (De hecho, ***inciso: ¿qué es lo que realmente está haciendo el encoder...***, *se te ocurre qué podríamos hacer con el encoder una vez entrenado todo el modelo...*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "luUMMVZ0rtMS"
      },
      "outputs": [],
      "source": [
        "vocab_size = 5000 # Número de tokens de nuestro vocabulario, en este caso vamos a hacer que token = (conjunto caracteres separados por espacios)\n",
        "max_length = 50 # Las secuencias de entrada están fijadas a 50, podríamos haberlas fijado a...\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length) # Como no decimos nada split=\"whitespace\", o sea la tokenizacion mencionada\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length)\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es]) # Importante le añadimos el comienzo de secuencia y el final para que sepa donde empieza y para que aprenda cuando se acaba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxiguIiTrtMT",
        "outputId": "913bbec9-3531-40ac-cfb7-5f86b4d05358"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "text_vec_layer_en.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRDoz0NOrtMT",
        "outputId": "db0b081a-8af6-4037-daf1-a164ce0e8554"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "text_vec_layer_es.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKXkImlH5Sdd"
      },
      "source": [
        "Veamos cómo codifica algunas de las setencias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH1Nz_po5iV4",
        "outputId": "c6f7b4c8-3da2-49b1-8227-44e9d1b5628b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our boss doesn't let us take coffee breaks.(8), '=>', Nuestro jefe no nos deja hacer una pausa para el café.(11)\n",
            "Vectorizacion sin embedding de la entrada al encoder\n",
            "tf.Tensor(\n",
            "[ 115  961   70  159   98  101  284 3192    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0], shape=(50,), dtype=int64)\n",
            "Vectorizacion sin embedding de la entrada al decoder\n",
            "tf.Tensor(\n",
            "[   2  228  888    7   76  440   52   18 4226   29   10  243    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0], shape=(50,), dtype=int64)\n",
            "Vectorizacion del target\n",
            "tf.Tensor(\n",
            "[ 228  888    7   76  440   52   18 4226   29   10  243    3    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0], shape=(50,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "origen = randint(0,len(sentences_es)-3)\n",
        "for i in range(origen, origen+1):\n",
        "    print(f\"{sentences_en[i]}({len(sentences_en[i].split())}), '=>', {sentences_es[i]}({len(sentences_es[i].split())})\")\n",
        "    print(\"Vectorizacion sin embedding de la entrada al encoder\",text_vec_layer_en(sentences_en[i]), sep = \"\\n\")\n",
        "    print(\"Vectorizacion sin embedding de la entrada al decoder\", text_vec_layer_es(f\"startofseq {sentences_es[i]}\"), sep = \"\\n\")\n",
        "    print(\"Vectorizacion del target\", text_vec_layer_es(f\"{sentences_es[i]} endofseq\"), sep = \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGVInDEhrtMT"
      },
      "source": [
        "Construímos los datasets de entrenamiento y validación, teniendo encuenta que encoder y decoder reciben entradas ligeramente diferentes y que el targert debe contener el endofseq (que es un token que debe predecir el modelo, es decir debe predecir cuando acaba la frase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6-vrcZDyrtMT"
      },
      "outputs": [],
      "source": [
        "X_train = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_en[100_000:])\n",
        "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
        "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6XplSzgrtMT"
      },
      "source": [
        "Y ahora sí, comenzamos con la definición (funcional del modelo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tB_pShS-rtMT"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KdCZZOiSrtMU"
      },
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "O6GgJ1xrrtMU"
      },
      "outputs": [],
      "source": [
        "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings) # IMPORTANTE obtenemos los estados de salida del encoder que es lo que...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "BIHHLh7prtMU"
      },
      "outputs": [],
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state) # ... realmente vamos a pasar al decoder para el primer token (<sos>) de la secuencia de guía (que en el entrenamiento es la de target desplazada)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FVaTrH1TrtMU"
      },
      "outputs": [],
      "source": [
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\") # La salida es una softmax con tantas neuronas como términos en el vocabulario, es decir estamos prediciendo el índice de cada palabra de la respuesta. Luego tendremos que decodificarlo para obtener la palabra real\n",
        "Y_proba = output_layer(decoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-W99DourtMW"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly a couple hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZT7TWtjrtMW",
        "outputId": "b4f65269-db62-49de-d2fd-9fbca8e5b22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3125/3125 [==============================] - 130s 38ms/step - loss: 4.0131 - accuracy: 0.3204 - val_loss: 3.0751 - val_accuracy: 0.4263\n",
            "Epoch 2/10\n",
            "3125/3125 [==============================] - 98s 31ms/step - loss: 2.5753 - accuracy: 0.4874 - val_loss: 2.3035 - val_accuracy: 0.5316\n",
            "Epoch 3/10\n",
            "3125/3125 [==============================] - 99s 32ms/step - loss: 1.8873 - accuracy: 0.5857 - val_loss: 1.9262 - val_accuracy: 0.5922\n",
            "Epoch 4/10\n",
            "3125/3125 [==============================] - 100s 32ms/step - loss: 1.4669 - accuracy: 0.6559 - val_loss: 1.7511 - val_accuracy: 0.6240\n",
            "Epoch 5/10\n",
            "3125/3125 [==============================] - 100s 32ms/step - loss: 1.1831 - accuracy: 0.7080 - val_loss: 1.6768 - val_accuracy: 0.6391\n",
            "Epoch 6/10\n",
            "3125/3125 [==============================] - 102s 33ms/step - loss: 0.9721 - accuracy: 0.7511 - val_loss: 1.6663 - val_accuracy: 0.6426\n",
            "Epoch 7/10\n",
            "3125/3125 [==============================] - 98s 32ms/step - loss: 0.8075 - accuracy: 0.7873 - val_loss: 1.6872 - val_accuracy: 0.6461\n",
            "Epoch 8/10\n",
            "3125/3125 [==============================] - 100s 32ms/step - loss: 0.6730 - accuracy: 0.8182 - val_loss: 1.7266 - val_accuracy: 0.6471\n",
            "Epoch 9/10\n",
            "3125/3125 [==============================] - 101s 32ms/step - loss: 0.5656 - accuracy: 0.8446 - val_loss: 1.7793 - val_accuracy: 0.6451\n",
            "Epoch 10/10\n",
            "3125/3125 [==============================] - 98s 31ms/step - loss: 0.4794 - accuracy: 0.8669 - val_loss: 1.8481 - val_accuracy: 0.6410\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cff7018ca90>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPtEaBMqrtMW"
      },
      "source": [
        "Una vez entrenado el modelo, la traducción tiene un poco de miga.   \n",
        "\n",
        "El decoder espera que le pasemos una secuencia guía (el teacher), que es la función que hacía la secuencia target desplazada uno en el entrenamiento.  \n",
        "\n",
        "Lo que vamos a hacer es ir prediciendo palabra a palabra introduciendo como guía la última predicción hasta llegar a que el modelo devuelva el carácter de fin de secuencia y en ese momento devolvemos la \"traducción\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "adTniqXUrtMW"
      },
      "outputs": [],
      "source": [
        "def translate(sentence_en):\n",
        "    translation = \"\"\n",
        "    for word_idx in range(max_length):\n",
        "        X = np.array([sentence_en])  # encoder input\n",
        "        X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
        "        y_probs = model.predict((X, X_dec))\n",
        "        y_proba = y_probs[0, word_idx]  # last token's probas\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "        predicted_proba = round(float(y_proba[predicted_word_id]),3)\n",
        "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
        "        if predicted_word == \"endofseq\":\n",
        "            break\n",
        "        translation += \" \" + predicted_word\n",
        "        print(f\"{translation}({predicted_proba})\")\n",
        "    return translation.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnh_m6khrtMW"
      },
      "source": [
        "Probemos con algo sencillo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "CKivIVBkrtMW",
        "outputId": "c0c6c80a-6208-4ac6-bcb8-a2815c666094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            " me(0.987)\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            " me gusta(0.996)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            " me gusta el(0.993)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            " me gusta el fútbol(0.963)\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta el fútbol'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "translate(\"I like soccer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDJfrl0mNeaS"
      },
      "source": [
        "Y si cambiamos un poco las palabras...:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "G27He7G0NdPh",
        "outputId": "0b912a74-bc3f-461c-afb7-cca5b61fad70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n",
            " adoro(0.662)\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            " adoro los(0.544)\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            " adoro los [UNK](0.904)\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            " adoro los [UNK] de(0.64)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " adoro los [UNK] de [UNK](0.569)\n",
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'adoro los [UNK] de [UNK]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "translate(\"I love Real Madrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UU9czJjrtMW"
      },
      "source": [
        "Bien!!! Pero qué ocurre si le pedimos algo más largo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "sbpR3btrrtMX",
        "outputId": "8cf2bc24-2638-4fd3-91c0-ff8c247469f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            " me(0.942)\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            " me gusta(0.987)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            " me gusta como(0.265)\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            " me gusta como los(0.535)\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            " me gusta como los peces(0.511)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me gusta como los peces y(0.974)\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            " me gusta como los peces y yo(0.162)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            " me gusta como los peces y yo prefiero(0.711)\n",
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta como los peces y yo prefiero'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ov8c7UzrtMX"
      },
      "source": [
        "Vamos a ver mejoras que además nos vayan adelantando conceptos para llegar a los LLN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RreDpT8rtMX"
      },
      "source": [
        "## Bidirectional RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zudtQcm1rtMX"
      },
      "source": [
        "Una red recurrente bidireccional es la que lee la secuencia tanto de izquierda a derecha como de derecha a izquierda y procesa ambas secuencias en conjunto. Ojo: la secuencia de entrada.\n",
        "\n",
        "En general es como tener una capa que mira en un sentido y otra en el otro y concatenar luego sus salidas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSNm3wlTrtMX"
      },
      "source": [
        "<img src=\"https://github.com/Jaimegrp/DS_Online_Oct23/blob/main/05_Deep_Learning/Sprint_21/02_NLP_y_Texto/img/bidirectionalrnn.jpg?raw=1\" alt=\"Bidirectional RNN\" width=\"700\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzxTTMFzrtMX"
      },
      "source": [
        "¿Por qué y para qué? Porque, por ejemplo, hay frases que para traducirlas necesitas ver que viene después, como en el caso de los adjetivos en inglés que anteceden al nombre y de los sinónimos en un idioma que no coinciden necesariamente con los sinónimos en otro: the left arm, the left party, they left the restaurant...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kdGZV5FrtMX"
      },
      "source": [
        "Para crear un capa recurrente bidireccional, se hace lo siguiente (encapsular una recurrente en una más genérica denominada Bidirectional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "a4fFw2scrtMY"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_state=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrN_tlVyrtMY"
      },
      "source": [
        "En el caso del decoder no podemos hacer lo mismo porque en el target mirar al futuro sí es hacer trampa (recordemos que hasta ahora le pasamos la palabra que correspondería a la palabra esperada anterior, o sea no hacemos trampa), y por tanto no serviría para predecir algo que no hubiera visto (de hecho no podríamos construir una entrada para predecir de forma correcta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jle372OvrtMY"
      },
      "source": [
        "Pero, la recurrente bidireccional produce el doble de estados ocultos. Como se trata de una lstm tendremos dos hidden_state y dos cell_state, aunque el decoder sólo espera dos (porque es otra LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6UyFduHrtMY"
      },
      "source": [
        "Lo que hacemos es concatenarlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "X-JWEi8GrtMb"
      },
      "outputs": [],
      "source": [
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n",
        "                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "532Ve8sgrtMb"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly a couple hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZadV-ZArtMc",
        "outputId": "eafa253c-b06b-4366-fc7f-ade467caef0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3125/3125 [==============================] - 153s 43ms/step - loss: 2.9885 - accuracy: 0.4632 - val_loss: 2.0485 - val_accuracy: 0.5789\n",
            "Epoch 2/10\n",
            "3125/3125 [==============================] - 112s 36ms/step - loss: 1.6005 - accuracy: 0.6437 - val_loss: 1.6344 - val_accuracy: 0.6420\n",
            "Epoch 3/10\n",
            "3125/3125 [==============================] - 111s 36ms/step - loss: 1.1880 - accuracy: 0.7125 - val_loss: 1.5029 - val_accuracy: 0.6653\n",
            "Epoch 4/10\n",
            "3125/3125 [==============================] - 111s 36ms/step - loss: 0.9493 - accuracy: 0.7579 - val_loss: 1.4839 - val_accuracy: 0.6711\n",
            "Epoch 5/10\n",
            "3125/3125 [==============================] - 114s 36ms/step - loss: 0.7765 - accuracy: 0.7948 - val_loss: 1.5025 - val_accuracy: 0.6736\n",
            "Epoch 6/10\n",
            "3125/3125 [==============================] - 111s 35ms/step - loss: 0.6425 - accuracy: 0.8251 - val_loss: 1.5419 - val_accuracy: 0.6729\n",
            "Epoch 7/10\n",
            "3125/3125 [==============================] - 114s 36ms/step - loss: 0.5382 - accuracy: 0.8505 - val_loss: 1.5942 - val_accuracy: 0.6723\n",
            "Epoch 8/10\n",
            "3125/3125 [==============================] - 110s 35ms/step - loss: 0.4561 - accuracy: 0.8719 - val_loss: 1.6605 - val_accuracy: 0.6693\n",
            "Epoch 9/10\n",
            "3125/3125 [==============================] - 114s 37ms/step - loss: 0.3926 - accuracy: 0.8882 - val_loss: 1.7231 - val_accuracy: 0.6660\n",
            "Epoch 10/10\n",
            "3125/3125 [==============================] - 110s 35ms/step - loss: 0.3421 - accuracy: 0.9019 - val_loss: 1.7896 - val_accuracy: 0.6627\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cfef5fb4130>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# extra code — completes the model and trains it\n",
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(decoder_outputs)\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "qUG81j9YrtMc",
        "outputId": "80d25260-a728-4fd1-95bb-b9c9a8620ec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            " me(0.965)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            " me gusta(0.999)\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            " me gusta el(0.914)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me gusta el fútbol(1.0)\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta el fútbol'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "translate(\"I like soccer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "-NF2SQovUKdb",
        "outputId": "aed2bf41-a3b0-4e46-cc30-7ae007db9798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            " me(0.553)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me gusta(0.988)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me gusta jugar(0.604)\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            " me gusta jugar a(0.521)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            " me gusta jugar a la(0.627)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            " me gusta jugar a la escuela(0.507)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me gusta jugar a la escuela y(0.995)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta jugar a la escuela y a(0.58)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me gusta jugar a la escuela y a pescar(0.342)\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me gusta jugar a la escuela y a pescar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-PZrYYArtMc"
      },
      "source": [
        "Otra posible optimización es lo que se denomina Beam Search. Se deja a modo de ejercicio para entenderlo y una referencia explicativa:\n",
        "https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzKUaOHortMc"
      },
      "source": [
        "## Beam Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDpeeJ1ArtMc"
      },
      "source": [
        "This is a very basic implementation of beam search. I tried to make it readable and understandable, but it's definitely not optimized for speed! The function first uses the model to find the top _k_ words to start the translations (where _k_ is the beam width). For each of the top _k_ translations, it evaluates the conditional probabilities of all possible words it could add to that translation. These extended translations and their probabilities are added to the list of candidates. Once we've gone through all top _k_ translations and all words that could complete them, we keep only the top _k_ candidates with the highest probability, and we iterate over and over until they all finish with an EOS token. The top translation is then returned (after removing its EOS token).\n",
        "\n",
        "* Note: If p(S) is the probability of sentence S, and p(W|S) is the conditional probability of the word W given that the translation starts with S, then the probability of the sentence S' = concat(S, W) is p(S') = p(S) * p(W|S). As we add more words, the probability gets smaller and smaller. To avoid the risk of it getting too small, which could cause floating point precision errors, the function keeps track of log probabilities instead of probabilities: recall that log(a\\*b) = log(a) + log(b), therefore log(p(S')) = log(p(S)) + log(p(W|S))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QQm-RZ8urtMc"
      },
      "outputs": [],
      "source": [
        "# extra code – a basic implementation of beam search\n",
        "\n",
        "def beam_search(sentence_en, beam_width, verbose=False):\n",
        "    X = np.array([sentence_en])  # encoder input\n",
        "    X_dec = np.array([\"startofseq\"])  # decoder input\n",
        "    y_proba = model.predict((X, X_dec))[0, 0]  # first token's probas\n",
        "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
        "    top_translations = [  # list of best (log_proba, translation)\n",
        "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
        "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
        "    ]\n",
        "\n",
        "    # extra code – displays the top first words in verbose mode\n",
        "    if verbose:\n",
        "        print(\"Top first words:\", top_translations)\n",
        "\n",
        "    for idx in range(1, max_length):\n",
        "        candidates = []\n",
        "        for log_proba, translation in top_translations:\n",
        "            if translation.endswith(\"endofseq\"):\n",
        "                candidates.append((log_proba, translation))\n",
        "                continue  # translation is finished, so don't try to extend it\n",
        "            X = np.array([sentence_en])  # encoder input\n",
        "            X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
        "            y_proba = model.predict((X, X_dec))[0, idx]  # last token's proba\n",
        "            for word_id, word_proba in enumerate(y_proba):\n",
        "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
        "                candidates.append((log_proba + np.log(word_proba),\n",
        "                                   f\"{translation} {word}\"))\n",
        "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
        "\n",
        "        # extra code – displays the top translation so far in verbose mode\n",
        "        if verbose:\n",
        "            print(\"Top translations so far:\", top_translations)\n",
        "\n",
        "        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n",
        "            return top_translations[0][1].replace(\"endofseq\", \"\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jndVFH6qrtMd",
        "outputId": "ed7117ad-c0cf-4f81-c6e1-8cb5a3d54233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 34ms/step\n",
            " me(0.721)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            " me encantan(0.779)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me encantan los(0.98)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me encantan los gatos(0.913)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            " me encantan los gatos y(0.966)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " me encantan los gatos y perros(0.451)\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me encantan los gatos y perros'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# extra code – shows how the model making an error\n",
        "sentence_en = \"I love cats and dogs\"\n",
        "translate(sentence_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "7eKZpXLortMd",
        "outputId": "dc189b17-6ed7-49d6-ffaf-f478c29bfae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "Top first words: [(-0.32662123, 'me'), (-1.6167213, 'yo'), (-3.2102604, 'adoro')]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Top translations so far: [(-0.5769473, 'me encantan'), (-1.8351828, 'yo amo'), (-1.8400223, 'me gustan')]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Top translations so far: [(-0.5968081, 'me encantan los'), (-1.9051129, 'yo amo a'), (-2.3993626, 'me gustan los')]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Top translations so far: [(-0.6874618, 'me encantan los gatos'), (-1.9064289, 'yo amo a los'), (-2.4903858, 'me gustan los gatos')]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Top translations so far: [(-0.7221249, 'me encantan los gatos y'), (-1.992027, 'yo amo a los perros'), (-2.5181458, 'me gustan los gatos y')]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Top translations so far: [(-1.5178745, 'me encantan los gatos y perros'), (-2.0904906, 'yo amo a los perros y'), (-2.5989978, 'me encantan los gatos y a')]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Top translations so far: [(-1.5178753, 'me encantan los gatos y perros endofseq'), (-2.3524384, 'yo amo a los perros y los'), (-2.6120892, 'me encantan los gatos y a los')]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Top translations so far: [(-1.5178753, 'me encantan los gatos y perros endofseq'), (-2.4165716, 'yo amo a los perros y los gatos'), (-2.6453223, 'me encantan los gatos y a los perros')]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Top translations so far: [(-1.5178753, 'me encantan los gatos y perros endofseq'), (-2.4173539, 'yo amo a los perros y los gatos endofseq'), (-2.6453905, 'me encantan los gatos y a los perros endofseq')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me encantan los gatos y perros'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# extra code – shows how beam search can help\n",
        "beam_search(sentence_en, beam_width=3, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHZA7cMKrtMd"
      },
      "source": [
        "The correct translation is in the top 3 sentences found by beam search, but it's not the first. Since we're using a small vocabulary, the \\[UNK] token is quite frequent, so you may want to penalize it (e.g., divide its probability by 2 in the beam search function): this will discourage beam search from using it too much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNF-QP37rtMd"
      },
      "source": [
        "## Mecanismos de Atencion (Attention mechanisms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52TxgFCRrtMd"
      },
      "source": [
        "Como mejora a este tipo de arquitecturas, en 2014 (Dzmitry Bahdanau y colegas, et al. que se dice) introdujeron una mejora sustancial a la arquitectura de Encoder-Decoders.\n",
        "\n",
        "La idea detrás del mecanismo es pasarle al decoder más información de la secuencia de entrada y no sólo los estados ocultos producidos por el último elemento (el primero y el úlitmo en el caso de bidireccionales). ¿Qué información? Pues algo así como la palabra que más le aporte en cada momento. Por ejemplo que cuando al decoder le toque producir fútbol en la traducción de I like soccer, reciba \"soccer\" (en concreto la salida del enconder a la palabra \"soccer\").  \n",
        "\n",
        "Supongamos que estamos traduciendo frases como:\n",
        "\n",
        "I like soccer  \n",
        "I like Rain Man  \n",
        "you like The Bridge  \n",
        "we like Marta  \n",
        "  \n",
        "Para los dos primeras el decoder iría traduciendo:\n",
        "(Me) gusta ... y la idea es que las entradas \"soccer\", \"Rain\" + \"Man\", \"The\" + \"Bridge\", \"Marta\" aporten más en ese instante...  \n",
        "\n",
        "Entonces al decoder tendré que pasarle todas las palabras de la frase (en concreto la salida de cada una de estas del encoder) y que exista un mecanismo que le diga en función de lo que lleva cuál de las entradas debe considerar más (aquí nos fijamos en la siguiente, pero para traducir \"Me\" es mejor que se fije en la primera, el pronombre, para traducir \"gusta\", igual)  \n",
        "\n",
        "Lo interesante no es considerar solo una palabra de entrada, sino una combinación pesada de estas (una combinación lineal -> \"The\" + \"Bridge\")\n",
        "\n",
        "Es decir, volviendo a nuestras entradas del decoder:\n",
        "\n",
        "* Antes d1: [h(e2),c(2),emb(\"<start_of_sequence>\")], ahora da1 (la a es de attention): [h(e2),c(e2), (coef1 * e1 + coef2 * e2 + coef3 * e3)] y probablemente todos los coef se aproximarían a cero\n",
        "* Antes d2: [emb(\"Me\"),$h_d$(d1),$c_d$(d1)], ahora da2: [emb(\"Me\"),$h_d$(d1),$c_d$(d1), (coef21 * e1 + coef22 * e2 + coef23 * e3)] y probablemente coef21 >> coef22 y coef23\n",
        "* Antes d3: [emb(\"gusta\"),$h_d$(d2),$c_d$(d2)], ahora da3: [emb(\"gusta\"),$h_d$(d1),$c_d$(d1), (coef31 * e1 + coef32 * e2 + coef33 * e3) ] y coef31 y coef32 serán altos y coef33 bajo o nulo\n",
        "\n",
        "\n",
        "__¿Y cómo le digo en cuál debe fijarse más? Pues como en los embeddings... que lo aprenda :-) (o sea tendré una Attention Layer)__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSJSezrertMd"
      },
      "source": [
        "Muy bien, y cómo se hace ese \"que lo aprenda\": Dos mecanismos de Atencion (aditiva y multiplicativa), pero el multiplicativo ha superado al aditivo y de hecho la Attention Layer de keras hace Attention multiplicativa y además al final la predicción se hace a partir de la salida de la capa de atención.\n",
        "\n",
        "Gráficamente:\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/encoder_decoder_with_attention.jpg?raw=1\" alt=\"Encoder_Decoder with Attention\" width=\"700\" />\n",
        "\n",
        "\n",
        "Intuitivamente al poner la capa de atención al final está configurando toda la red (toda incluidos los embeddings) para que \"memorice\" la relación estadística entre las posiciones de salida y las de entrada en diferentes situaciones. Y luego ya nosotros a eso le llamamos atención, porque es verdad que cuando llega el momento de \"Me gusta el...\", ha memorizado que las posiciones que deben aportar más es donde haya salidas que generalmente pertenecen a nombres. (pero él ni sabe que son nombres, ni que está traduciendo ni nada,...)\n",
        "\n",
        "\n",
        "El siguiente paso sería aumentar la memoria y olvidarse de las recurrencias (y del problema de traducir) :-)... los Transformers, pero antes... prestémosle Atención al traductor con atención"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6sxAEvslrtMd"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True)) # Ahora necesitamos todas las salidas del Encoder por eso return_sequences = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "SL9ZaWurrtMe"
      },
      "outputs": [],
      "source": [
        "# extra code – this part of the model is exactly the same as earlier\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n",
        "                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)\n",
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5v4cs87rtMe"
      },
      "source": [
        "And finally, let's add the `Attention` layer and the output layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "abYzi6gjrtMe"
      },
      "outputs": [],
      "source": [
        "attention_layer = tf.keras.layers.Attention()\n",
        "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(attention_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncoE2lSKrtMe"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly a couple hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "aICWvCoBrtMe",
        "outputId": "6c8e3dcc-2d3d-4019-e13c-51960dc6c083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 504/3125 [===>..........................] - ETA: 2:34 - loss: 4.8474 - accuracy: 0.2374"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-ff5da409c058>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n\u001b[1;32m      4\u001b[0m               metrics=[\"accuracy\"])\n\u001b[0;32m----> 5\u001b[0;31m model.fit((X_train, X_train_dec), Y_train, epochs=10,\n\u001b[0m\u001b[1;32m      6\u001b[0m           validation_data=((X_valid, X_valid_dec), Y_valid))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gnOroYdrtMe"
      },
      "outputs": [],
      "source": [
        "translate(\"I like your shoes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8upfzZJ3Bsc"
      },
      "outputs": [],
      "source": [
        "translate(\"I love you\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rIi2lK7rtMe"
      },
      "outputs": [],
      "source": [
        "beam_search(\"I like soccer and also going to the beach\", beam_width=3,\n",
        "            verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx3jPpJOrtMf"
      },
      "source": [
        "## Y si la solución es simple y llanamente establecer relacion los elementos de la secuencia entre sí.... ***Attention Is All You Need: The Transformer Architecture***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eghXhvLprtMf"
      },
      "source": [
        "Y en 2017, alguien de una empresa..., publicó un paper en el que se presentaba en sociedad una arquitectura que sólo necesitaba de mecanismos de Atenttion y capas Densas para hacer NMT como el mejor.\n",
        "\n",
        "OJO, esta sección es a título ilustrativo... Pero molón\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/transformers.webp?raw=1\" alt=\"Encoder_Decoder with Attention\" width=\"700\" />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snQivuXHrtMf"
      },
      "source": [
        "Antes de asustarse, lo que realmente hace esta arquitectura es ser las superconvolucional con memoria de las secuencias de texto... Vamos por partes:\n",
        "\n",
        "__La parte del encoder__ al incluir el mecanismo de atención (y ese positional encoding) lo que está haciendo es aprender las relaciones posicionales de las palabras del \"lenguaje\" de entrada (sí está memorizando y caracterizando las relaciones entre todas las palabras y sus posiciones de entrada), está haciendo un embedding de la información posicional. Luego concatena la información posicional con la de entrada y aprende a relacionarla lo mejor posible para esa secuencia. En tiempo de inferencia diríamos que para cada secuencia está analizandola sintácticamente, gramaticalmente, etc, etc y luego la devuelve... Pero nunca le hemos pasado información ni sintáctica, ni gramatical, ni nada.\n",
        "\n",
        "__La parte del decoder__ primero hace lo mismo que el encoder pero con el \"lenguaje\" de destino, se aprende y caracteriza todas las relaciones posicionales de las sentencias de ese \"lenguaje\". Combina ese embedding por secuencia con la secuencia de destino y se lo pasa a la siguiente capa de atención que ahora memoriza y caracteriza todas las relaciones posicionales entre las sentencias procesadas y enriquecidas del lenguaje destino y las sentencias procesadas y enriquecidas del lenguaje de origen. Y luego el feedfoward es el que realmente mezcla todo (mezclará toda la información de la secuencia, vease que se va transmitiendo, las posiciones y las posiciones con el lenguaje origen).\n",
        "\n",
        "Para aumentar la memoria y dar más relaciones lo que hacemos es que la capa de atención en realidad son muchas capas de atención en paralelo y acumular modulos de atención (como acumulábamos capas convolucionales en una red convolucional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl91Sdu4rtMf"
      },
      "source": [
        "A título ilustrativo un __capa multihead de atención__:\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/multihead_attention.png?raw=1\" alt=\"Multihead Attention Layer\" width=\"700\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh_6C3mWrtMf"
      },
      "source": [
        "Las capas linear son capas densas sin función de activación, tienen pesos entrenables -> Memoria (en estos pesos está la memoria de las características posicionales)\n",
        "Y las capas head fuerzan que las linear (tantas como cabezas*3) aprendan un número de relaciones \"poscionales\" que dependen del número de cabezas (si quiero que apredan más relaciones posicionales -> Más cabezas)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zna3md12rtMf"
      },
      "source": [
        "Y para terminar hay N modulos (en el paper de 2017, 6 por Encoder y 6 por Decoder) apilados, ¿por qué? Aquí una idea intuitiva es la misma que en las convolucionales, para que pueda memorizar relaciones posicionales más complicadas (y tener más memoria entrenable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3ar9UqqrtMf"
      },
      "source": [
        "## Large Language Models (Pretrained Transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt8wIErgrtMf"
      },
      "source": [
        "Y en 2018, llegaron las arquitecturas que apilaban multihead attention layers pero ya olvidándose de decoders y encoders... Una sola columna con muchos módulos... pero con la magia de estar pre-entrenadas para una tarea (generalmente adivina cuál es la siguiente palabra de la sentencia (GPT) o de cada sentencia te voy a ocultar 2 palabras, adivina cuales son (BERT)).\n",
        "\n",
        "En esencia, están memorizando todas las relaciones que existen entre las palabras de un lenguaje... Por eso cada vez se hacen más grandes (para memorizar más) y tienen más parámetros... y necesitan más que les des de comer (si quieres tenerlo todo representado)... (Imagina que le pudieras dar para entrenar a un red convolucional todas las imágenes posibles que existen)\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/gpt2_vs_BERT.jpg?raw=1\" alt=\"Multihead Attention Layer\" width=\"800\" />\n",
        "\n",
        "Luego estos modelos preentrenados se adaptan (fine-tunning), o sea se hace transfer learning, a otros tipos de problemas (clasificación, sentiment analysis, question and answers, y ahora chats, texto generativo).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOi7vkFOAClz"
      },
      "source": [
        "## Instruct LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_8qCUXkAE_r"
      },
      "source": [
        "Hoy en día, lo que realmente se ha puesto \"de moda\" no es emplear los LLM como los vistos hasta ahora, sino los fine-tuned y más concretamente los basados en una aproximación denominada \"Instruct LLM\" (partiendo de un paper de OpenAI):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yY38EYRAbDw"
      },
      "source": [
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/rlhf.jpg?raw=1\" alt=\"Instruct LLM\" width=\"800\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrzskW7CrtMg"
      },
      "source": [
        "Han surgido decenas de modelos, propietarios y abiertos, los más destacados (que puedes usar):  \n",
        "[OpenAI, ChatGPT](https://chat.openai.com/)  \n",
        "[Google, Gemini](https://gemini.google.com/?hl=es)  \n",
        "[Mistral, Mistral MoE](https://mistral.ai/)  \n",
        "[Meta, Llama-2](https://ai.meta.com/blog/5-steps-to-getting-started-with-llama-2/)  \n",
        "[Hugging Face, miles y miles de modelos](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)  \n",
        "[Preplexity AI, integra GPT-4 y Claude (no accesible directamente en España](https://www.perplexity.ai/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaczsP3-AHDx"
      },
      "source": [
        "### Curiosidades y simlitudes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEa5hcGfrtMg"
      },
      "source": [
        "Veamos algunas cosas curiosas:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "i2yCjWSA9mD0"
      },
      "outputs": [],
      "source": [
        "import pyperclip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "dIVTtMEH8jQT"
      },
      "outputs": [],
      "source": [
        "prompt = \"Dame una lista de los siete mejores arquitectos de la historia o por lo menos de los más famosos y dame un ejemplo de sus mejores obras o de las más conocidas\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1EKhHry9Rso",
        "outputId": "e6fe07b7-362e-4d61-ef3d-5da391c4906d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dae una lita de los site meores aruitectos de la hitoria o por lo meos de los más faosos y dae un ejmplo de sus meores obas o de las más coocidas\n"
          ]
        }
      ],
      "source": [
        "new_prompt = []\n",
        "for token in prompt.split():\n",
        "  new_token = \"\"\n",
        "  if len(token) > 3:\n",
        "    new_token = token[0:2] + token[3:]\n",
        "  else:\n",
        "    new_token = token\n",
        "  new_prompt.append(new_token)\n",
        "print(\" \".join(new_prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX9qBfO3_TOu",
        "outputId": "469deb75-043d-4e70-fcd8-3c60d6bbcfe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conocidas más las de o obras mejores sus de ejemplo un dame y famosos más los de menos lo por o historia la de arquitectos mejores siete los de lista una Dame\n"
          ]
        }
      ],
      "source": [
        "new_prompt = prompt.split()[::-1]\n",
        "print(\" \".join(new_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaGP8wTYALTJ"
      },
      "source": [
        "### Uso programático:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AdgtZc2BULN"
      },
      "source": [
        "*Ver el otro notebook*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qhj4QK6rtMg"
      },
      "source": [
        "### Bonus: PROGRAMANDO UN TRANSFORMER (GPT-2 like)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZwnnhSVrtMh"
      },
      "source": [
        "En las siguientes celdas se muestra como construir un transformer para nuestro traductor... para el que quiera jugar (extraido del Hands-On...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5UExD9rtMh"
      },
      "source": [
        "### Positional encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "CrOa1E-Qx7la",
        "outputId": "8ff806c1-cb1e-48b3-f408-bc3b1c55a712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Missing required positional argument",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-285e1b8c6dd3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miterable_params\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_iterable_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_dispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Missing required positional argument"
          ]
        }
      ],
      "source": [
        "tf.shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2TZgJSZrtMi"
      },
      "outputs": [],
      "source": [
        "max_length = 50  # max length in the whole training set\n",
        "embed_size = 128\n",
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "pos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
        "batch_max_len_enc = tf.shape(encoder_embeddings)[1]\n",
        "encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\n",
        "batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
        "decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwLLeq-TrtMi"
      },
      "source": [
        "Alternatively, we can use fixed, non-trainable positional encodings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hjsMZ3xrtMi"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
        "        p, i = np.meshgrid(np.arange(max_length),\n",
        "                           2 * np.arange(embed_size // 2))\n",
        "        pos_emb = np.empty((1, max_length, embed_size))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
        "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_max_length = tf.shape(inputs)[1]\n",
        "        return inputs + self.pos_encodings[:, :batch_max_length]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vArtPEfzrtMi"
      },
      "outputs": [],
      "source": [
        "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
        "encoder_in = pos_embed_layer(encoder_embeddings)\n",
        "decoder_in = pos_embed_layer(decoder_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkyBTS9HrtMi"
      },
      "outputs": [],
      "source": [
        "# extra code – this cells generates and saves Figure 16–9\n",
        "figure_max_length = 201\n",
        "figure_embed_size = 512\n",
        "pos_emb = PositionalEncoding(figure_max_length, figure_embed_size)\n",
        "zeros = np.zeros((1, figure_max_length, figure_embed_size), np.float32)\n",
        "P = pos_emb(zeros)[0].numpy()\n",
        "i1, i2, crop_i = 100, 101, 150\n",
        "p1, p2, p3 = 22, 60, 35\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(9, 5))\n",
        "ax1.plot([p1, p1], [-1, 1], \"k--\", label=\"$p = {}$\".format(p1))\n",
        "ax1.plot([p2, p2], [-1, 1], \"k--\", label=\"$p = {}$\".format(p2), alpha=0.5)\n",
        "ax1.plot(p3, P[p3, i1], \"bx\", label=\"$p = {}$\".format(p3))\n",
        "ax1.plot(P[:,i1], \"b-\", label=\"$i = {}$\".format(i1))\n",
        "ax1.plot(P[:,i2], \"r-\", label=\"$i = {}$\".format(i2))\n",
        "ax1.plot([p1, p2], [P[p1, i1], P[p2, i1]], \"bo\")\n",
        "ax1.plot([p1, p2], [P[p1, i2], P[p2, i2]], \"ro\")\n",
        "ax1.legend(loc=\"center right\", fontsize=14, framealpha=0.95)\n",
        "ax1.set_ylabel(\"$P_{(p,i)}$\", rotation=0, fontsize=16)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.hlines(0, 0, figure_max_length - 1, color=\"k\", linewidth=1, alpha=0.3)\n",
        "ax1.axis([0, figure_max_length - 1, -1, 1])\n",
        "ax2.imshow(P.T[:crop_i], cmap=\"gray\", interpolation=\"bilinear\", aspect=\"auto\")\n",
        "ax2.hlines(i1, 0, figure_max_length - 1, color=\"b\", linewidth=3)\n",
        "cheat = 2  # need to raise the red line a bit, or else it hides the blue one\n",
        "ax2.hlines(i2+cheat, 0, figure_max_length - 1, color=\"r\", linewidth=3)\n",
        "ax2.plot([p1, p1], [0, crop_i], \"k--\")\n",
        "ax2.plot([p2, p2], [0, crop_i], \"k--\", alpha=0.5)\n",
        "ax2.plot([p1, p2], [i2+cheat, i2+cheat], \"ro\")\n",
        "ax2.plot([p1, p2], [i1, i1], \"bo\")\n",
        "ax2.axis([0, figure_max_length - 1, 0, crop_i])\n",
        "ax2.set_xlabel(\"$p$\", fontsize=16)\n",
        "ax2.set_ylabel(\"$i$\", rotation=0, fontsize=16)\n",
        "save_fig(\"positional_embedding_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP0A5FmgrtMj"
      },
      "source": [
        "### Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaNUlZBNrtMj"
      },
      "outputs": [],
      "source": [
        "N = 2  # instead of 6\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "n_units = 128  # for the first Dense layer in each Feed Forward block\n",
        "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
        "Z = encoder_in\n",
        "for _ in range(N):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZtmcl8lrtMj"
      },
      "outputs": [],
      "source": [
        "encoder_outputs = Z  # let's save the encoder's final outputs\n",
        "Z = decoder_in  # the decoder starts with its own inputs\n",
        "for _ in range(N):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQW4XffvrtMj"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly 2 or 3 hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmC-BfkgrtMj"
      },
      "outputs": [],
      "source": [
        "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThoiaaMvrtMj"
      },
      "outputs": [],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXaMFZc8rtMj"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_xsKX3ortMk"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v80zKJcfrtMk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "X = tf.range(10)\n",
        "print(X)\n",
        "X_2 = tf.round(tf.random.uniform(shape= (5,4))*100,2)\n",
        "print(X_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkkmOmjSrtMk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrW-VMOZrtMk"
      },
      "outputs": [],
      "source": [
        "prueba = tf.data.Dataset.from_tensor_slices(X_2)\n",
        "list(prueba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snlgwCvkrtMk"
      },
      "outputs": [],
      "source": [
        "X_nested = {\"a\": ([1,2,3],[\"a\",\"b\",\"c\"]), \"b\": ([\"Hola\",\"adios\",\"ok\"],[1.1,2.2,-4])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trrt61nXrtMl"
      },
      "outputs": [],
      "source": [
        "for elemento in tf.data.Dataset.from_tensor_slices(X_nested):\n",
        "    print(elemento[\"a\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxsaz8aFrtMl"
      },
      "outputs": [],
      "source": [
        "prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evlI2GaxrtMl"
      },
      "outputs": [],
      "source": [
        "prueba.repeat(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTRdag8frtMm"
      },
      "outputs": [],
      "source": [
        "for i in prueba.repeat(3).batch(3, drop_remainder= True):\n",
        "    print(i)\n",
        "    print(type(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71aH9gY1rtMm"
      },
      "outputs": [],
      "source": [
        "prueba.map(lambda x: x+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdlX-jrErtMm"
      },
      "outputs": [],
      "source": [
        "for i,dato in enumerate(prueba.map(lambda x: x+1)):\n",
        "    print(dato)\n",
        "    print(i)\n",
        "    if i>3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aRHJlWnrtMm"
      },
      "outputs": [],
      "source": [
        "for i,dato in enumerate(prueba.map(lambda x: x+1)):\n",
        "    print(dato)\n",
        "    print(tf.reduce_sum(dato, axis = 0))\n",
        "    print(i)\n",
        "    if i>3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Spl5NUJXrtMn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}