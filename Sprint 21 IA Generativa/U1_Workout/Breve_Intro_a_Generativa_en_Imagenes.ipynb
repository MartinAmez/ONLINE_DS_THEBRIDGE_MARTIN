{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrldIDfWQD1D"
      },
      "source": [
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/img/cabecera.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLKdUu59QD1E"
      },
      "source": [
        "## Una (muy) breve introducción a la IA Generativa en Imágenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Vamos a emplear colab así, que allí nos vamos..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/Jaimegrp/DS_Online_Oct23/blob/main/05_Deep_Learning/Sprint_21/01_Imagenes/Breve_Intro_a_Generativa_en_Imagenes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Montamos Drive, o lo intentamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Comprobamos la versión del python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comprobamos la versión de sklearn ≥ 1.0.1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "y de TensorFlow ≥ 2.8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Algunos ajustes para cuando lancemos nuestro entrenamiento de redes GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from time import time, sleep\n",
        "\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comprobación de si tenemos GPU a mano, porque si no ya te puedes ir a tomar un desayuno, comida o cena."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
        "              \"accelerator.\")\n",
        "    if \"kaggle_secrets\" in sys.modules:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@markdown Chequea el tipo de GPU y la memoria virtual disponible\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizaremos el FMNIST, ese gran amigo nuestro de ropas y complementos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extra code – loads, scales, and splits the fashion MNIST dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
        "X_train_full = X_train_full.astype(np.float32) / 255\n",
        "X_test = X_test.astype(np.float32) / 255\n",
        "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
        "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y un poco de código para poder pintar las imágenes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_multiple_images(images, n_cols=None):\n",
        "    n_cols = n_cols or len(images)\n",
        "    n_rows = (len(images) - 1) // n_cols + 1\n",
        "    if images.shape[-1] == 1:\n",
        "        images = images.squeeze(axis=-1)\n",
        "    plt.figure(figsize=(n_cols, n_rows))\n",
        "    for index, image in enumerate(images):\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(image, cmap=\"binary\")\n",
        "        plt.axis(\"off\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr4dT2NmQD1E"
      },
      "source": [
        "Lo primero un poco de historia sobre las redes generativas, principalmente de imágenes. Todo empieza cuando queremos encontrar representaciones de las imágene denominadas \"latentes\" (vamos reduciditas y compactas):\n",
        "\n",
        "* **Autoencoders**\n",
        "* **GANs** (Generative Adversarial Networks)\n",
        "* **Diffusion** (o más correctamente DDPM, Denoising Diffusion Probabilistic Model, que entrenados con condicionamiento de texto dan lugar a: Stable XL, Midjourney, DALL-E, etc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijggZPBIQD1F"
      },
      "source": [
        "## Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wNj4tjxQD1F"
      },
      "source": [
        "Son **redes neuronales** que constan de un ***encoder*** (o red de reconocimiento) y un ***decoder*** (red de generación) generalmente simétricos **que se entrena con una tarea semi-supervisada o auto-supervisada**. Básicamente, reproducir la entrada, sí así como suena.\n",
        "\n",
        "La idea es que el encoder aprenda una representación (embedding, espacio latente, etc) de la entrada que luego pueda servir para ser reconstruida. Es un extractor genérico de features :-). Luego te lo llevas al problema que quieras, pero además con un poco de magia también puede inventarse entradas (pero eso no lo vamos a ver)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5-IJRO0QD1F"
      },
      "source": [
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/img/simple_autoencoder.png?raw=1\" alt=\"Diagrama de autoencoder\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Figura: Stacked Autoencoder (Stacked -> Varias capas ocultas en Encoder y Decoder)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY-N4Dl-QD1F"
      },
      "source": [
        "Lo que se hace es \"fastidiar\" al enconder (pocas neuronas, meterle ruido, dropout incluso en inferencia,etc) lo que le fuerza a encontrar una representación lo más eficiente posible.\n",
        "\n",
        "Se usan para varias cosas:\n",
        "- PCA\n",
        "- Lantent Analysis\n",
        "- Feature Extraction. Ejemplo, autoetiquetador: Entrenas el autoencoder, te llevas el encoder como feature extraction para etiquetar imágenes\n",
        "- Quitar ruído a imágenes\n",
        "- Generación de imágenes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejemplo: Análisis Latente/Reducción de dimensionalidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a reducir las 28*28 features de las imágenes aplandas del FMNIST a 30 y luego vamos a ver cómo de buena es la clusterización, reduciendo a su vez linealmente esas 30 a 2 y pintando el cluster resultante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Ejemplo: Reducción de dimensionalidad, representación latente (análisis latente)\n",
        "\n",
        "tf.random.set_seed(42)  \n",
        "stacked_encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "])\n",
        "stacked_decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(28 * 28),\n",
        "    tf.keras.layers.Reshape([28, 28])\n",
        "])\n",
        "stacked_ae = tf.keras.Sequential([stacked_encoder, stacked_decoder])\n",
        "\n",
        "stacked_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "history = stacked_ae.fit(X_train, X_train, epochs=20,\n",
        "                         validation_data=(X_valid, X_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observa la forma de construirlo precisamente para tener acceso al encoder luego por separado... De hecho vamos a ver que tal desde el punto de vista de \"compresión\"/\"reconstrucción\" de imágenes (recuerda el ejercicio de la PCA y las cámaras del casino)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_reconstructions(model, images=X_valid, n_images=5, origin = 0):\n",
        "    n_images = n_images if (origin + n_images) < images.shape[0] else max(1,images.shape[0]-origin)\n",
        "    reconstructions = np.clip(model.predict(images[origin:origin + n_images]), 0, 1)\n",
        "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
        "    for image_index in range(n_images):\n",
        "        plt.subplot(2, n_images, 1 + image_index)\n",
        "        plt.imshow(images[origin + image_index], cmap=\"binary\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
        "        plt.imshow(reconstructions[image_index], cmap=\"binary\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "plot_reconstructions(stacked_ae, origin = np.random.randint(0,X_valid.shape[0]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hmmm, la compresión es \"mala\", pero veamos que pasa si proyectamos las 30 nuevas dimensiones en 2 (ojo, no vamos a usar PCA sino otra aproximación, t-SNE, que viene de perlas para pintar cluster de alta dimensión... Jaime, ya podías haberlo antes :-S), y pintamos los clusters creados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "X_valid_compressed = stacked_encoder.predict(X_valid)\n",
        "tsne = TSNE(init=\"pca\", learning_rate=\"auto\", random_state=42)\n",
        "X_valid_2D = tsne.fit_transform(X_valid_compressed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=\"tab10\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y esto de regalo (adaptado de Aurelien Geron que a su vez lo adapta de los [ejemplos de sklearn](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "cmap = plt.cm.tab10\n",
        "Z = X_valid_2D\n",
        "Z = (Z - Z.min()) / (Z.max() - Z.min())  # normalize to the 0-1 range\n",
        "plt.scatter(Z[:, 0], Z[:, 1], c=y_valid, s=10, cmap=cmap)\n",
        "image_positions = np.array([[1., 1.]])\n",
        "for index, position in enumerate(Z):\n",
        "    dist = ((position - image_positions) ** 2).sum(axis=1)\n",
        "    if dist.min() > 0.02: # if far enough from other images\n",
        "        image_positions = np.r_[image_positions, [position]]\n",
        "        imagebox = mpl.offsetbox.AnnotationBbox(\n",
        "            mpl.offsetbox.OffsetImage(X_valid[index], cmap=\"binary\"),\n",
        "            position, bboxprops={\"edgecolor\": cmap(y_valid[index]), \"lw\": 2})\n",
        "        plt.gca().add_artist(imagebox)\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evolución de los Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ns-5g0aQD1F"
      },
      "source": [
        "Los auntoencoders han evolucionado desde el modelo sencillo (también llamado stacked, porque hay varias capas en encoder y decoder) hasta los VAE (Variational Autoencoders), y de por medio han aparecido los convolucionales que usan capas convolucionales. \n",
        "\n",
        "Estas dos variantes los VAE y el uso de capas convolucionales están detras de los dos grandes saltos dentro de la generativa de imágenes. \n",
        "\n",
        "Empezando por los **VAE**, muy resumidamente, introducen un truco que es que lo que alimenta al decoder no es directamente la salida de las neuronas del encoder sino que a partir de una salida se genera otra que se fuerza a que pertenezca a una distribución gaussiana con la anterior (Eihn, ¿qué?). Es como si al introducirle una imagen le estuvieramos diciendo que no es esa imagen sino un prototipo de imagen y que aprenda variantes de la misma. \n",
        "\n",
        "Luego podemos separar el decoder y e introducirle entradas aleatorias (que sigan una distribución gaussiana) y será capaz de inventarse imágenes con cierto sentido.\n",
        "\n",
        "\n",
        "Por otro lado, se puede construir un Autoencoder con **capas convolucionales a la entrada del enconder**, pero para poder invertir las capas y hacer una construcción simétrica **en el decoder, es necesario usar unas capas denominadas capas convolucionales transpuestas**. Y en cuanto tuvimos esto a alguien se le ocurrio poner a competir a dos autoencoders y nacieron las GAN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FucUwcyUQD1F"
      },
      "source": [
        "Para entender un poco más los GAN sólo un inciso, las capas convolucionales transpuestas, son capas que reciben la salida de otras capas convolucionales y devuelven la salida de sus filtros con la resolución de entrada a esas otras capas convolucionales\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/img/transposed_Deconvolution.webp?raw=1\" alt=\"Inversa y Transpuesta de una convolucional\" height=\"200\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q99WNxU3QD1F"
      },
      "source": [
        "En un autoencoder, si en el encoder (Conv2D) hay convolucionales en el decoder hay transpuestas (Conv2DTranspose)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZshQ3YiNQD1F"
      },
      "source": [
        "## GANs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U43S3OeuQD1F"
      },
      "source": [
        "Son dos redes neuronales con misiones contrapuestas. Una (la denominada Generator) tiene como objetivo crear instancias falsas o sintéticas que la otra (denominada Discriminator) debe clasificar como verdaderas, mientas que la misión del Discriminator es la de aprender cuáles son las falsas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPHL2b3bQD1G"
      },
      "source": [
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/img/GAN.jpg?raw=1\" alt=\"Diagrama sencillo GAN\" width=\"1000\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9wzPnNWQD1G"
      },
      "source": [
        "Para entenderlas un poco mejor, merece la pena conocer cómo se entrenan (porque no es cómo en las redes vistas hasta ahora):\n",
        "1. Primero al batch de imágenes reales se le añade otro batch de imágenes falsas. Estas imágenes falsas se crean metiendo ruido gaussiano (tiramos muchas veces un gran dado \"especial\" que sigue una distribución estadística gaussiana, las caras del dado no tienen las mismas probabilidades de salir :-)). Tiramos tantas veces nuestro \"dado gaussiano\" como \"features\" (llamadas codings) queramos darle de comer. Nuestro Generador creará tantas imágenes como haga falta haciendo pasar la entrada aleatoria por sus capas (sí, claro, las primeras veces será un manchurrón porque los pesos del Generador no están entrenados) y las mezclamos con el batch de imágenes reales.\n",
        "2. A las imagénes reales les asignamos la clase 1, por ejemplo, y a las falsas la clase 0. Ahora entrenamos sólo el Discriminador (que las primeras veces lo tendrá chupao).\n",
        "3. \"congelamos\" el discriminador (es decir evitamos que sus capas sean entrenadas en la siguiente parte)\n",
        "4. Ahora sí, usamos toda la red Generator seguido de Discriminator para entrenar (pero como hemos hecho 3. sólo se entrenará el Generator, ingenioso, eh?)\n",
        "5. Y, ¿cómo? Pasamos otras \"tiradas de dados\" como entradas (si pasasemos las mismas que en 1 y 2 entonces el Discriminator tendría ventaja que ya las ha visto con su etiqueta de verdad), el Generator, generará las mismas imágenes fake, pero a estas las etiquetasmos como reales y las pasamos por el Discriminator. Ahora cada vez que el Discriminator diga que es fake (mucho al principio) los pesos de las capas del Generator se irán configurando para poder engañar al Discriminator...\n",
        "\n",
        "Y voilá... Veámoslo con código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IPbJEmZpKzu"
      },
      "source": [
        "This project requires Python 3.7 or above:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hA5xhT_yLhN"
      },
      "source": [
        "# Generative Adversarial Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N31KxgksyLhN"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "\n",
        "codings_size = 30\n",
        "\n",
        "# Creamos un modelo para el discriminador y un modelo para el generador. Una vez entrenado este último nos permitirá crear caras (eso sí, de forma aleatoria, no podremos guiar el proceso)\n",
        "\n",
        "Dense = tf.keras.layers.Dense\n",
        "generator = tf.keras.Sequential([\n",
        "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    Dense(28 * 28, activation=\"sigmoid\"),\n",
        "    tf.keras.layers.Reshape([28, 28]) # Convertimos la salida del generador en \"imágenes\" de 28x28\n",
        "])\n",
        "discriminator = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    Dense(1, activation=\"sigmoid\") # Un clasificador binario en DL como los que hemos visto varias veces\n",
        "])\n",
        "gan = tf.keras.Sequential([generator, discriminator])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm4Uk3MTyLhN"
      },
      "outputs": [],
      "source": [
        "# Primer truco para entrenar, creamos un modelo para el discriminador y un modelo con generador y discriminador, pero con las capas del discriminador \"congeladas\"\n",
        "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
        "discriminator.trainable = False\n",
        "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQoPjJthyLhN"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "cxrUUhrjyLhN",
        "outputId": "0a48066c-3a48-4536-c8f6-2c15473ea685"
      },
      "outputs": [],
      "source": [
        "def train_gan(gan, dataset, batch_size, codings_size, n_epochs):\n",
        "    entrenamiento_time = []\n",
        "    generator, discriminator = gan.layers\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_t_zero = time()\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}\")  # extra code\n",
        "        for X_batch in dataset:\n",
        "            # phase 1 - training the discriminator\n",
        "            noise = tf.random.normal(shape=[batch_size, codings_size]) # Generamos unas tiradas de dados para hacer las imágenes falsas\n",
        "            generated_images = generator(noise) # Generamos las imágenes fake pasándoloas por el Generator tal cual esté en ese momento, (a medida que se vayan generando batches irá mejoran)\n",
        "            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0) # Mezclamos falsas (generated_images) y reales (X_batch)\n",
        "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size) # Asignamos 0 a las falsas y 1 a las reales\n",
        "            discriminator.train_on_batch(X_fake_and_real, y1) # Entrenamos, utilzando el método especial de entrenar en un sólo batch y aprovechamos que los modelos se entrenan progresivamente :-), no se pierden los pesos al volver a entrenar\n",
        "            # phase 2 - training the generator\n",
        "            noise = tf.random.normal(shape=[batch_size, codings_size]) # Nuevas tiradas para no dar ventaja al Discriminator, y que serán la entrada a toda la GAN\n",
        "            y2 = tf.constant([[1.]] * batch_size) # Preparamos la trampa para el Discriminator, le vamos a decir que todas son reales (por eso tenemos que congelar los pesos del Discriminator sino aprendería a decir que sí a todas las imágenes fake)\n",
        "            gan.train_on_batch(noise, y2) # Utilizamos el modelo GAN, que en realidad sólo está entrenando las capas del Generator\n",
        "        # extra code — plot images during training\n",
        "        entrenamiento_time.append(time()-epoch_t_zero)\n",
        "        plot_multiple_images(generated_images.numpy(), 8) # Para ver como el Generator va mejorando\n",
        "        clear_output(wait = True)\n",
        "        print(\"Dur epoca:\", entrenamiento_time[-1])\n",
        "        plt.show()\n",
        "    print(\"Duracion entrenamiento:\", np.mean(entrenamiento_time))\n",
        "train_gan(gan, dataset, batch_size, codings_size, n_epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcf52U9IQD1J"
      },
      "source": [
        "Y ahora que ya lo hemos entrenado, juguemos un poco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6eRh-_jQD1J",
        "outputId": "8bafe54a-748d-459c-846d-51c9eed3d41d"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "\n",
        "codings = tf.random.normal(shape=[batch_size, codings_size])\n",
        "generated_images = generator.predict(codings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "NrLzMxweyLhO",
        "outputId": "1d52f47b-d9fd-48b9-83e6-f9a18a65f0b7"
      },
      "outputs": [],
      "source": [
        "# extra code – this cell generates and saves Figure 17–15\n",
        "plot_multiple_images(generated_images, 8)\n",
        "#save_fig(\"gan_generated_images_plot\", tight_layout=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ChOjv0wQD1J"
      },
      "source": [
        "## Evolución de las GAN: StyleGANs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbxrpeJSQD1K"
      },
      "source": [
        "__Deep Convolutional GANs (DCGAN)__: Las GAN que hemos visto usan capas densas, lo siguiente fue incluir capas Convolucionaes Transpuestas en el Generator y Convolucionales al Discriminator y hacer algunos truquis para que el entrenamiento fuera estable (no se llegaran a \"acuerdos\" entre Discrimiantor y Generator, como, por ejemplo, lo que se conoce como Mode Collapse).\n",
        "\n",
        "__Progressive DCGAN__: En 2018, ese año, los investigadores de Nvidia introdujeron nuevas mejoras, siendo la principal la arquitectura de la red es dinámica en entrenamiento (se van añadiendo capas convolucionales a medida que se entrena) y otros elementos para hacerla estable.\n",
        "\n",
        "__StyleGANs__: No mucho después, introdujeron cambios que dieron un salto cualitativo destacado. Básicamente cambiaron la arquitectura del Generator de forma que ahora tenía a su vez dos partes: Mapping Network y Synthesis Network (ambas entrenables)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7efjHCJcQD1K"
      },
      "source": [
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/img/styleGAN.webp?raw=1\" drawing = \"Diagrama de un Generator de una StyleGAN\" width =\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QW6HZU9QD1K"
      },
      "source": [
        "Lo diferencial, es que la mapping network es como un superembedding de las imágenes (Creado antes de sintetizar las imágenes) que da un vector por imagen (w, también style vector) base y luego este vector se pasa por las capas de transformación (A) que generan otros subvectores que ya sí entran el módulo de synthsis que genera la imagen aleatoria.  \n",
        "\n",
        "Lo potente además de que se generen imágenes cada vez más realistas es que una vez entrenado este Generator, se pueden tocar las cajas A y sobre una misma imagen generada podemos cambiar las características (el color del pelo, la edad, el humor, etc). De hecho también podemos cambiar la imagen de partida tocando el vector w.\n",
        "  \n",
        "Lo que no podemos así tal cual es tocar foto que no haya sido generada por el sistema (tened en cuenta que se le dan tiradas aleatorias de dados para comer). Por supuesto hay aplicaciones que son capaces de sacar el vector w de una imagen dada y luego aplicar una red de synthesis preentrenadas.\n",
        "\n",
        "\n",
        "* [Alquileres](https://thisrentaldoesnotexist.com)\n",
        "* [Caras](https://thispersondoesnotexist.com)\n",
        "* [Which Face is Real](http://www.whichfaceisreal.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUCR2FrdyLhP"
      },
      "source": [
        "# Diffusion Models..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyvKvczoil-7"
      },
      "source": [
        "Muy resumidadmente, los modelos de difusion (no son GAN), son redes a las que se les pasa como entrenamiento imágenes a las que se les ha aplicado ruido de forma progresiva (a cada pixel se le ha ido añadiendo un número aleatorio pequeñito progresivamente) y el ruido. La red aprende que dada una imagen con ruido, cuál es el ruido.  \n",
        "\n",
        "Posteriormente, para predecir, se le da \"ruido\" (nunca hubo imagen, y llamémoslo manchurón del principio), pero no se le indica que sea así y la red predice el que cree que sería el ruido de haber una imagen real detrás, y vamos quitando capas y sale la imagen (hallucinated que se dice técnicamente) a la que habríamos tenido que añadir el ruido que ha \"supuesto\" la red para que saliera el manchurrón del principio.\n",
        "\n",
        "Imagen_X + un poco de manchurrón -> imagen_1  (la red tiene que aprender el \"poco de manchurrón\")  \n",
        "\n",
        "imagen1 + otro poco de manchurrón -> imagen_2 (la red tiene que aprender el \"otro poco de manchurrón\")\n",
        "  \n",
        "imagen2 + más poco manchurrón -> imagen_3 (la red tiene que aprender \"más poco manchurrón\")  \n",
        "\n",
        "....\n",
        "....\n",
        "...\n",
        "imagen3999 + poquito final de manchurrón -> imagen4000  (que ahora será un buen manchurrón completo indistinguible para el ojo humano) -> la red tiene que aprender \"poquito final de manchurrón\".\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/img/diffusion_forward.png?raw=1\" alt =\"Proceso forward de diffusion\" width = \"800\"/>\n",
        "\n",
        "Y así con millones de imágenes...\n",
        "\n",
        "El modelo está preparado para...\n",
        "\n",
        "Supermanchurrón (sin imagen detrás) model.predict(Supermanchurrón) -> poquito_final (la red predice aunque no tenga sentido, rember?)  \n",
        "\n",
        "Supermanchurrón-un poquito_final -> model.predict(Supermanchurrón-un_poquito_final) -> un poquito menos de manchurrón  \n",
        "\n",
        "Supermanchurrón-un poquito_final - un poquito menos de manchurrón -> model.predict....  \n",
        "  \n",
        "\n",
        ".....\n",
        "Supermanchurrón sin todos los 4000 preditct manchurroncitos -> Lo que sea que dios quiera, pero siempre en en el espacio latente de las imágenes que le he dado es decir una combinación de las imágenes de partida -> MAGIA!!!  \n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/img/denoising_diffusion.jpg?raw=1\" alt = \"Proces de quitar ruido en Diffusion\" width = \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVaIOZ1BQD1K"
      },
      "source": [
        "### ... y aprendizaje condicional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MySrS9rIQD1K"
      },
      "source": [
        "Además a estos sitemas se les ha añadido entrenamiento con aprendizaje condicional. Básicamente el aprendizaje condicional se da cuando a nuestro datase de entrada le añadimos \"condiciones\" (features extra). En este caso el modelo actual (Mar2023) es Stable Diffusion, que ha sido entrenado con millones y millones de imágenes a las que se ha condicionado con un texto descriptivo.  \n",
        "\n",
        "A la hora entrenar a la imagen además de añadir el ruido se le añade una \"vectorización\" del texto de caption (algo así como su embedding) de forma que a la hora de hacer el denoising la red no \"intuye\" el poquito de manchurrón que tiene la imagen, sino el poquito de manchurrón condicionado al texto que se le dé de entrada.\n",
        "\n",
        "Para saber más (jugando con ello, que ya podéis perfectamente)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjlERDHqQD1K"
      },
      "source": [
        "https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwzEbsLBQD1K"
      },
      "source": [
        "https://keras.io/examples/generative/finetune_stable_diffusion/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*NOTA: El finetune de Keras no funciona directamente porque el dataset, dibujos de Pokemon con descripción, tuvieron que cerrarlo a petición de los dueños de los derechos... El script sí funciona, pero necesitas alimentarlo con una buena panda de imágenes proporcionadas por ti*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BONUS: Deep Convolutional GAN (DCGAN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "\n",
        "codings_size = 100\n",
        "\n",
        "generator = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(7 * 7 * 128),\n",
        "    tf.keras.layers.Reshape([7, 7, 128]),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2,\n",
        "                                    padding=\"same\", activation=\"relu\"),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2,\n",
        "                                    padding=\"same\", activation=\"tanh\"),\n",
        "])\n",
        "discriminator = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\",\n",
        "                        activation=tf.keras.layers.LeakyReLU(0.2)),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"same\",\n",
        "                        activation=tf.keras.layers.LeakyReLU(0.2)),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "gan = tf.keras.Sequential([generator, discriminator])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
        "discriminator.trainable = False\n",
        "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_dcgan = X_train.reshape(-1, 28, 28, 1) * 2. - 1 # Necesario porque el generador tiene una tanh (-1,1) como función de activación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X_train_dcgan)\n",
        "dataset = dataset.shuffle(1000)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "train_gan(gan, dataset, batch_size, codings_size, n_epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO4XYPxFQD1K"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Breve_Intro_a_Generativa_en_Imagenes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "nav_menu": {
      "height": "381px",
      "width": "453px"
    },
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
